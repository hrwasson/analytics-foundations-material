--- 
title: "Analytics Foundations Materials"
author: "Hannah Wasson"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
description: "Within this document are the answers for all the labs and breakout sessions for Analytics Foundations"
---



<!--chapter:end:index.Rmd-->

# The pool of tears


<!--chapter:end:02-tears.Rmd-->

# A caucus-race and a long tale

<!--chapter:end:03-race.Rmd-->

---
title: "Lab1_AF"
author: "Hannah Wasson"
date: "2024-06-25"
output: html_document
---

#Installing and using normtemp package to answer questions... 

```{r}
#install.packages('UsingR')
library(UsingR)
data(normtemp)
```

Use the normtemp dataset to answer the following:
a. Determine the following statistics for the variable temperature:
b. Does temperature appear to be normally distributed? no. 

```{r}
library(ggplot2)

min(normtemp$temperature)
max(normtemp$temperature)
mean(normtemp$temperature)
sd(normtemp$temperature)
ggplot(data = normtemp, aes(x = temperature)) + geom_histogram()
```

c. Create box plots for temperature. Are there any outliers? Display
a reference line at 98.6
Does the median body temperature seem to be 98.6 degrees? yes. 


```{r}
ggplot(data = normtemp, aes(x = temperature)) + geom_boxplot()+ geom_vline(xintercept=98.6) 
```
Using the Ameshousing dataset from our in-class examples, run some
distributional analysis on Sale_Price, Log(Sale_Price), and Gr_Liv_Area.
a. Create histograms of these three variables.
- Overlay a kernel density estimator of the variables.

```{r}
library(AmesHousing)
ames <- make_ordinal_ames()

ggplot(data = ames, aes(x = Sale_Price)) + geom_histogram(aes(y=..density..), fill = 'lightblue') + geom_density()

ggplot(data = ames, aes(x = log(Sale_Price))) + geom_histogram(aes(y=..density..), fill = 'orange') + geom_density()

ggplot(data = ames, aes(x = Gr_Liv_Area)) + geom_histogram(aes(y=..density..), fill = 'lightgreen') + geom_density()



```


b. Create a QQ Plot for both Sale_Price and Log(Sale_Price). Based on
these exploratory procedures, which version of the price information
(Sale_Price or Log(Sale_Price)) would you say is closer to being normally
distributed?

```{r}
ggplot(data = ames, aes(sample = Sale_Price), color = 'lightblue') + geom_qq() + stat_qq() + stat_qq_line() + labs(title = 'Sales Price QQ Plot')
ggplot(data = ames, aes(sample = log(Sale_Price)), color = 'orange') + geom_qq() + stat_qq() + stat_qq_line()+ labs(title = 'log(Sales Price) QQ Plot')
ggplot(data = ames, aes(sample = Gr_Liv_Area), color = 'lightgreen') + geom_qq() + stat_qq() + stat_qq_line()+ labs(title = 'Gr_Liv_Area QQ Plot')
```

log(Sale_Price) is the closest to being normally distributed. 


# Using the Ameshousing dataset from our in-class examples, determine the
# following:

a. What type of variables are each of these columns (Nominal, Ordinal, or
Continuous/Quantitative)? Keep in mind that the way they are
represented in the R dataset may not be appropriate, so you should make
this determination using your own judgement based on the data you are
looking at.
- Overall_Qual -- Ordinal 
- Lot_Shape --  Ordinal
- Heating_QC -- Ordinal
- Lot_Area -- Quantitative


```{r}
str(ames$Overall_Qual)
str(ames$Lot_Shape)
str(ames$Heating_QC)
str(ames$Lot_Area)

```


<!--chapter:end:af_lab_1.Rmd-->

---
title: "af_lab_10"
author: "Hannah Wasson"
date: "2024-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Complete Example

## Provide your best LINEAR model for the Ames Housing data set. You should use your train data set to create the best model.  You cannot have more than 12 variables in the model (this includes polynomial terms and interaction terms).  ONLY one submission per team (if I see more than one, your team will NOT get credit for the assignment).

```{r}
library(tidyverse)
library(ggplot2)
library(AmesHousing)
library(estimatr)
ames <- make_ordinal_ames()
```

```{r}
set.seed(123)
#ames <- ames %>% mutate(id = row_number())
#train <- ames %>% sample_frac(0.7)
#test <- anti_join(ames, train, by = "id")
```

```{r}
summary(train)
names<- colnames(train)
numeric_data <- ames[, sapply(ames, is.numeric)]
cor_matrix <- cor(numeric_data)
names
```
Took out Gr_Liv_Area and it lowered all VIFs

full_model <- lmSale_Price ~ (Year_Built + TotRms_AbvGrd + Year_Remod_Add + Mas_Vnr_Area + First_Flr_SF  + Full_Bath  + Fireplaces + Garage_Area + Wood_Deck_SF + Open_Porch_SF, data = train)

```{r}
empty_model <- lm(Sale_Price ~ 1, data = train)
full_model <- lm(Sale_Price ~ (Year_Built + TotRms_AbvGrd + Year_Remod_Add + Mas_Vnr_Area + First_Flr_SF  + Full_Bath  + Fireplaces + Garage_Area + Wood_Deck_SF + Open_Porch_SF + House_Style + Bldg_Type), data = train)
for_model <- step(empty_model, 
                  scope = list(lower = empty_model, 
                               upper = full_model), 
                  direction = "both", k = qchisq(0.003, 1, lower.tail =  FALSE))

vif(for_model)
```

```{r}
#summary(for_model)
#plot(for_model)
#shapiro.test(for_model$residuals)
#boxcox(for_model)
pairs(train[, c('Sale_Price','Year_Built', 'TotRms_AbvGrd', 'Year_Remod_Add', 'Mas_Vnr_Area', 'First_Flr_SF', 'Full_Bath' , 'Fireplaces' , 'Garage_Area' , 'Wood_Deck_SF',  'Open_Porch_SF')])

Year_Built.c <- scale(train$Year_Built, scale = FALSE)

mod_1 <- lm(Sale_Price ~ Garage_Area + First_Flr_SF + Year_Remod_Add + Mas_Vnr_Area + 
    Fireplaces + TotRms_AbvGrd + Year_Built.c + I(Year_Built.c^2) + Wood_Deck_SF + 
    Full_Bath + Open_Porch_SF, data = train)



#plot(mod_1)
#summary(mod_1)
#boxcox(mod_1)

# robust - draws in variance if variance is changingi 
mod_2 <- lm_robust(Sale_Price ~ Lot_Area + First_Flr_SF + Year_Remod_Add + Mas_Vnr_Area + 
    Fireplaces + TotRms_AbvGrd + Year_Built.c + I(Year_Built.c^2) + Wood_Deck_SF + 
    Full_Bath + Open_Porch_SF + Overall_Qual, data = train, se_type = "HC1")

summary(mod_2)

fitted_values_mod2 <- mod_2$fitted.values
residuals_mod2 <- (fitted_values_mod2 - train$Sale_Price)
df <- data.frame(fitted_values_mod2, residuals_mod2)

#dim(df)
ggplot(data = df, aes(x = fitted_values_mod2, y = residuals_mod2)) + geom_point()
ggplot(data = df, aes(x = residuals_mod2)) + geom_histogram()
ggplot(data = df, aes(sample = residuals_mod2)) + geom_qq() + geom_qq_line()

#ggplot(mod_2, aes(x = .fitted, y = residuals_mod2)) + geom_point() + geom_hline(yintercept = 0)
```
```{r}
hat.cut=2*(length(coefficients(mod_2)))/nrow(train)
n.index=seq(1,nrow(train)) 
ggplot(mod_2,aes(x=n.index,y =hatvalues(mod_2)))+geom_point(color="blue")+geom_line(y=hat.cut)+labs(title = "Hat values",x="Observation",y="Hat Values") 

ggplot(mod_2,aes(x=hatvalues(mod_2),y=rstudent(mod_2)))+geom_point(color="orange")+geom_line(y=-3)+geom_line(y=3)+labs(title = "External Studentized Residuals",x="Hat values",y="Residuals") + geom_vline(xintercept = hat.cut)
```


We found a negative sales price 

```{r}

First_Flr_SF.c <- scale(train$First_Flr_SF, scale = FALSE)

mod_4 <- lm_robust(Sale_Price ~ I(Year_Built.c^3)  + I(Year_Built.c^2) + Year_Built.c + TotRms_AbvGrd + Year_Remod_Add + Mas_Vnr_Area + First_Flr_SF.c + I(First_Flr_SF.c^2)  + Full_Bath  + Fireplaces + Garage_Area + Wood_Deck_SF + Open_Porch_SF + House_Style + Bldg_Type, data = train, se_type = "HC2")

fitted_values_mod4 <- mod_4$fitted.values
residuals_mod4 <- (fitted_values_mod4 - train$Sale_Price)
df_4 <- data.frame(fitted_values_mod4, residuals_mod4)

pairs(train[, c('Sale_Price','Year_Built', 'TotRms_AbvGrd', 'Year_Remod_Add', 'Mas_Vnr_Area', 'First_Flr_SF', 'Full_Bath' , 'Fireplaces' , 'Garage_Area' , 'Wood_Deck_SF',  'Open_Porch_SF' , 'House_Style', 'Bldg_Type')])

ggplot(data = df_4, aes(x = fitted_values_mod4, y = residuals_mod4)) + geom_point()
ggplot(data = df_4, aes(x = residuals_mod4)) + geom_histogram()
ggplot(data = df_4, aes(sample = residuals_mod4)) + geom_qq() + geom_qq_line()

summary(mod_4)

#boxcox(mod_4)
```
```{r}
mod_5 <- lm(Sale_Price ~ Garage_Area + First_Flr_SF + Year_Remod_Add + House_Style + 
    Year_Built + Mas_Vnr_Area + Fireplaces + Bldg_Type + Wood_Deck_SF + 
    TotRms_AbvGrd, data = train)

summary(mod_5)
plot(mod_5)
shapiro.test(mod_5$residuals)
```

mod_6 <- lm(log(Sale_Price) ~ Lot_Area + Year_Built + Bldg_Type + Overall_Cond + Foundation + Central_Air + Gr_Liv_Area + Full_Bath + Neighborhood + Sale_Condition + Functional , data = train)

```{r}
Gr_Liv_Area.c <- scale(train$Gr_Liv_Area, scale = FALSE)

mod_6 <- lm(log(Sale_Price) ~ Lot_Area + Year_Built + Bldg_Type + Overall_Cond + Foundation + Central_Air + Gr_Liv_Area + Full_Bath + Neighborhood + Sale_Condition + Functional , data = train)

mod_7 <- lm(log(Sale_Price) ~ Lot_Area + Year_Built + Bldg_Type + Overall_Cond + Foundation + Central_Air + Gr_Liv_Area.c + I(Gr_Liv_Area.c ^ 2) + Full_Bath + Neighborhood + Sale_Condition + Functional , data = train)

plot(mod_6)
vif(mod_6)
shapiro.test(mod_6$residuals)

```

```{r}

mod_7 <- lm(Sale_Price ~ Bldg_Type + Lot_Area + Foundation + Central_Air + Gr_Liv_Area + Full_Bath + Neighborhood + Overall_Qual, data = train)

vif(mod_7)
summary(mod_7)
plot(mod_7)
shapiro.test(mod_7$residuals)
boxcox(mod_7)
```



```{r}
amelot.c <- scale (ameTrain$Lot_Area, scale = F)
greater.c <- scale (ameTrain$Gr_Liv_Area, scale = F)model <- lm(sqrt(Sale_Price) ~ Lot_Area + I(amelot.c^2) + Bldg_Type + Overall_Qual + Neighborhood + Foundation + Central_Air + Gr_Liv_Area + I(greater.c^2) + I(greater.c^3) + I(greater.c^4), data = ameTrain)print(shapiro.test(model$residuals))
summary(model)
plot(model)
```

# Final Model 

```{r}
mod_6 <- lm(log(Sale_Price) ~ Lot_Area + Year_Built + Bldg_Type + Overall_Cond + Foundation + Central_Air + Gr_Liv_Area + Full_Bath + Neighborhood + Sale_Condition + Functional , data = train)
plot(mod_6)


hat.cut=2*(length(coefficients(mod_6)))/nrow(train)
n.index=seq(1,nrow(train)) 
ggplot(mod_6,aes(x=n.index,y =hatvalues(mod_6)))+geom_point(color="blue")+geom_line(y=hat.cut)+labs(title = "Hat values",x="Observation",y="Hat Values") 

ggplot(mod_6,aes(x=hatvalues(mod_6),y=rstudent(mod_6)))+geom_point(color="orange")+geom_line(y=-3)+geom_line(y=3)+labs(title = "External Studentized Residuals",x="Hat values",y="Residuals") + geom_vline(xintercept = hat.cut)

```


<!--chapter:end:af_lab_10.Rmd-->

---
title: "AF_Lab2"
author: "Hannah Wasson"
date: "2024-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1. Revisit the NormTemp dataset from Lab 1, where we examined the observed mean body temperature (temperature) in comparison to the well-known “average” of 98.6.

# Perform a statistical test (alpha = 0.05) to determine whether this well- known number is actually the mean body temperature. What is your p-value? Explain in words what this p-value means. What is your conclusion?

```{r}
library(UsingR)
data(normtemp)

t.test(normtemp$temperature, mu = 98.6)

<!--chapter:end:af_lab_2.Rmd-->

---
title: "af_lab_3"
author: "Hannah Wasson"
date: "2024-06-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Use the dataset Garlic to perform an ANOVA. This dataset has information on the weight of garlic bulbs (bulbwt) for 32 different garlic plants. Each garlic plant was initially treated with 1 of 4 types of fertilizer. The purpose of the experiment is to determine whether or not there is any difference in the resulting bulb weight when a garlic plant is treated with a different fertilizer.

```{r}
garlic <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/garlic.csv')

str(garlic)
```
# Test the hypothesis that the means of bulb weight are equal regardless of the fertilizer used.
# - Verify that the assumption of normality in each sample you are testing. Are you comfortable with this assumption?
# - Verify that the groups satisfy the assumption of equal variance.
# - State your conclusion to the hypothesis test (alpha = 0.05)


First let's do EDA.. 

```{r}
library(ggplot2)
ggplot(data = garlic, aes(x = BulbWt, y = factor(Fertilizer), fill = factor(Fertilizer))) + geom_boxplot() + stat_summary(fun = mean, geom = 'point', size = 5, color = 'red') + scale_fill_brewer(palette = 'Blues') + theme_classic() + labs(title = 'Fertilizer vs. Bulb Weight', x = 'Bulb Weight', y = 'Fertilizer')

```
There does seem to be a significant difference in the fertilizer type and the bulb weight. 

Let's check for normality amongst the different types. 

```{r}
library(ggpubr)

mod <- aov(BulbWt ~ Fertilizer, data = garlic)
ggqqplot(residuals(mod))
shapiro.test(residuals(mod))
```

There is normality in the model. 

Let's check for variances... 

```{r}

ggplot(data = mod, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0)

```

Check using Levines test... 

```{r}
```


```{r}
library(stats)
library(car)
leveneTest(BulbWt~factor(Fertilizer), data = garlic)
```
We fail to reject the null and conclude that there are equal variances. 

So there is normality and equal variances. We can use the basic ANOVA. 

```{r}
anova_mod <- aov(BulbWt~Fertilizer, data = garlic)
summary(anova_mod)
```

With a alpha of 0.05 we can conclude that there is a significant difference in the means between fertilizers and bulb weight. 

# If you were to test if each type of fertilizer were different from each other fertilizer (pairwise), how many hypothesis tests would you be running?

If you are testing each type of fertilizer versus each other fertilizer you would be running 6 different comparisons. 

# If the probability of incorrectly rejecting a true null hypothesis is 0.05, and each hypothesis test is considered independent, what is the probability that you incorrectly reject at least one true hypothesis? How do we solve this problem when we’re performing post-hoc analysis in ANOVA?

1 - (1 - a )^n 
1 - (1 - 0.05)^6 
= 0.26

To solve this problem we will want to use the Tukey Games-Howell Dunn test. 

# Which fertilizers are statistically different from each other and which fertilizers do not appear to produce different bulb weights?

```{r}
garlic_aov = aov(BulbWt ~ factor(Fertilizer), data = garlic)
garlic_tukey = TukeyHSD(garlic_aov, las = 1)
print(garlic_tukey)
plot(garlic_tukey)
```
Based on the Tukey test, only fertilizers 4 and 1, and 4 and 3 produce significantly different bulb weights from one another. 3 & 1, 3 & 2, and 4 & 2 don't produce any different in bulb weights. 
Only two comparisons produce different bulbweights



# 3. The Bottle dataset contains observations from a factory that is producing plastic water bottles along 3 different assembly lines

```{r}
bottles <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bottle.csv')

str(bottles)
```


First lets do EDA...

```{r}
ggplot(data = bottles, aes(x = Units, y = factor(Line), fill = factor(Line))) + geom_boxplot() + stat_summary(fun = mean, geom = 'point', size = 5, color = 'red') + scale_fill_brewer(palette = 'Blues') + theme_classic() + labs(title = 'Number of Units Produced within Each Assembly Line', x = 'Units', y = 'Line')

```

It does appear that the number of units are statistically significant from one another. 

```{r}
qq_mod <- aov(Units ~ Line, data = bottles)

ggqqplot(residuals(qq_mod))
shapiro.test(residuals(qq_mod))

ggplot(data = bottles, aes(x = Units, fill = factor(Line))) + geom_histogram()  

```

We cannot conclude normality in the distribution of the residuals. We will go right to a Kruskal Wallis test. But first lets check the variances. 
```{r}
ggplot(data = qq_mod, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0)
leveneTest(Units~factor(Line), data = bottles)
```
We cannot conclude that the variances are equal either. 

```{r}
kruskal.test(Units ~ Line, data = bottles)
```
Since the p-value is less than 0.05 we will reject the null and conclude that there is a significant difference in the units of bottles produced based on the assembly line used. 

Lets check for comparison wise post ANOVA 

```{r}
library(dunn.test)

dunn.test(bottles$Units, bottles$Line, kw = T, method = "bonferroni")

```

We can't confirm that one line is better than the other but we can say that Line 3 is significantly different than lines 2 and 1. 


# The Trials dataset contains information from a clinical trial for blood pressure medicine.
```{r}
trials <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/trials.csv')
str(trials)

```

Perform EDA

```{r}
ggplot(data = trials, aes(x = BPChange, y = factor(Treatment), fill = factor(Treatment))) + geom_boxplot() + stat_summary(fun = mean, geom = 'point', size = 5, color = 'red') + scale_fill_brewer(palette = 'Blues') + theme_classic() + labs(title = 'Number of Units Produced within Each Assembly Line', x = 'Units', y = 'Line')
```

Based on the Exploratory Data Analysis there seems to be a significant difference in the means of the three different drugs. 

Let's look at normality and variances... 

```{r}
trials_mod <- aov(BPChange ~ Treatment, data = trials)

ggqqplot(residuals(trials_mod))
shapiro.test(residuals(trials_mod))

ggplot(data = trials_mod, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0)
leveneTest(BPChange~factor(Treatment), data = trials)

```

Cannot assume normality or equal variances. 

Must do a Kruskal Wallis non-parametric test. 

```{r}
kruskal.test(BPChange ~ Treatment, data = trials)
```

We can say that there is a significant change in location of the means for the various treatments and BP changes. 

Let's use the Wilcoxon test with a Bonferronin adjustment 

```{r}
library(PMCMRplus)
summary(manyOneUTest(x = trials$BPChange, g = as.factor(trials$Treatment), p.adjust.methods = 'bonferroni' ))
```
Only the new drug out performes the placebo in blood pressure change. 

<!--chapter:end:af_lab_3.Rmd-->

---
title: "af_lab_4"
author: "Hannah Wasson"
date: "2024-06-28"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fuel Economy Dataset

“The <http://fueleconomy.gov> website, run by the U.S. Department of Energy’s Office of Energy Efficiency and Renewable Energy and the U.S. Environmental Protection Agency, lists different estimates of fuel economy for passenger cars and trucks. For each vehicle, various characteristics are recorded such as the engine displacement or number of cylinders. Along with these values, laboratory measurements are made for the city and highway miles per gallon (MPG) of the car.”

Predictors extracted from the website include: EngDispl, NumCyl, Transmission, AirAspirationMethod, NumGears, TransLockup, TransCreeperGear, DriveDesc, IntakeValvePerCyl, ExhaustValvesPerCyl, CarlineClassDesc, VarValveTiming and VarValveLift. The response variable is FE, which is the unadjusted highway data

To access this data set, you will need to install the AppliedPrectiveModeling library. The following code will produce 3 data sets (cars2010, cars2011 and cars2012, which are the training, validation and test data sets, respectively).

```{r}
library(AppliedPredictiveModeling)
library(ggplot2)
data(FuelEconomy)
```

#### a. Generate scatter plots and correlations for the variables EngDispl, NumCyl, ExhaustValvesPerCyl and the VarValveTiming versus the target variable, FE.

**- Can linear relationships adequately describe these relationships?**

We can say that there is a linear relationship between FE/EngineDispl

\- Are there any outliers that you should investigate?

yes there are some outliers in the FE/EngineDispl that may have some weight on the linear relationship

**- What variable has the highest correlation with FE?**

Engine Display seems to have the highest correlation with FE.

**- What is the p-value for that correlation coefficient? Is it statistically significant at the 0.05 level? What can you conclude?**

The p-value of the correlation coefficient for this is 2.2e-16. yes, it is statistically significant meaning that we would reject the null meaning that there is a linear relationship. It doesn't tell you about the strength or the direction.

```{r}
cor(cars2010[, c('EngDispl', 'NumCyl', 'ExhaustValvesPerCyl', 'VarValveTiming', 'FE')])
pairs(cars2010[, c('EngDispl', 'NumCyl', 'ExhaustValvesPerCyl', 'VarValveTiming', 'FE')])
ggplot(data = cars2010, aes(x = EngDispl, y = FE, color = 'pink')) + geom_point() + stat_smooth(method = 'lm', formula =  y ~ x, color = 'black', show.legend = FALSE)
cor.test(cars2010$FE, cars2010$EngDispl)
```

#### **b. Generate correlations among all of the variables in the previously mentioned variables, minus the target, FE. Are there any notable relationships?**

Yes, there seems to be a notable relationship between engdispl and numcyl

```{r}
cor(cars2010[, c('EngDispl', 'NumCyl', 'ExhaustValvesPerCyl', 'VarValveTiming')])
pairs(cars2010[, c('EngDispl', 'NumCyl', 'ExhaustValvesPerCyl', 'VarValveTiming')])
```

Checking for linearity of the mean (no pattern in the fitted vs. residual plot), variance (residuals vs. fitted in the variance plot), and normality - All fail.

```{r}
check_cars <- lm(FE ~ EngDispl, data = cars2010)
ggplot(data = check_cars, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0)
plot(check_cars)
```

#### Fit a simple linear regression model with FE as the response variable and EngDispl as the predictor.

**- What is the value of the F Statistic and the associated p-value? How would you interpret this with regard to the null hypothesis?**

the F-statistic is 1803 and the p-value is 2.2e-16. Since the p-value is less than the alpha of 0.05 I would reject the null hypothesis and conclude that there is a significant linear relationship between engine display and fuel economy. The F-statistic tests whether or not the y-intercept Bo is equal to or not equal to 0. In this case it is not equal to 0.

**- Write the predicted regression equation.**

y = 50.5632 + -4.5209 X

For a one-unit increase in engine display we would expect fuel economy to decrease by -4.5209 units.

**- What is the value of R-square? How would you interpret this?**

The r-squared is 0.62. this represents the percentage of variation in the linear regression of FE explained by engine display.

```{r}
car_slm <- lm(FE ~ EngDispl, data = cars2010)
summary(car_slm)
```

## Ice Cream Dataset

The IceCream dataset has two columns, sales which gives the total daily sales of a local ice cream shop in hundreds of dollars, and temperature which reflects the daily high temperature.

```{r}
icecream <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/icecream.csv')
str(icecream)
```

#### Run a regression analysis predicting daily sales from temperature.

**- Are the errors of your model normally distributed? What evidence would you cite here?**

The following plots show the relationship between the means.

variance : residuals are equally distributed and there is no pattern - pass linearity : no pattern shown in the residuals vs. fitted values - pass normality : qq-plot is normally distributed - pass

```{r}
check_icecream <- lm(Sales ~ Temperature, data = icecream)
ggplot(data = check_icecream, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0)
ggplot(data = check_icecream, aes(x = .resid)) + geom_histogram()
plot(check_icecream)
```

**- Do you see evidence of any relationship between temperature and sales? What statistical evidence (think: p-value) would you cite here?**

Yes the correlation coefficient is 0.7845466 and the p-value is 1.572e-11. We can reject the null to conclude that there is a linear relationship between Temperature and Sales.

```{r}
cor.test(icecream$Temperature, icecream$Sales)
ggplot(data = icecream, aes(x = Sales, y = Temperature, fill = 'lightgreen')) + geom_point() + stat_smooth(formula = 'y ~ x', method = 'lm')
```

**- What is the parameter estimate for temperature in the model equation? Interpret this parameter using a sentence.**

the parameter estimate for temperature is 1.0901. This means that a one-unit increase in temperature is expected to increase ice cream sales by 1.0901 units.

```{r}
icecream_slm <- lm(Sales ~ Temperature, data = icecream)
summary(icecream_slm)
```

## Minneapolis Temperature Dataset

The dataset MinnTemp has information for the daily average temperature for a weather station in Minneapolis (when reading in this data set, be sure to tell it that the separator is a space, i.e. sep=” “). The variables temp and time provide the temperature and time measurements respectively. Time is measured in hours since the study began.

```{r}
minntemp <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/minntemp.csv', sep = " ")
```

#### b. Perform a regression analysis predicting temperature using the time variable.

**- Are the errors of your model normally distributed? What evidence** **would you cite here?**

Yes - the errors in the model are normally distributed based on the QQ-plot

**- Do you see violations of our assumptions for simple linear regression? If so, what problems do you see?**

Yes. Equal variances do not pass the assumptions there is not homogeneity in the residuals. Additionally, there is a pattern in the residuals so it does not pass linearity among residuals.

```{r}
check_minntemp <- lm(Temp ~ Time, data = minntemp)
cor.test(minntemp$Time, minntemp$Temp)
ggplot(data = check_minntemp, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0)
plot(check_minntemp)
```

**- Is there statistical evidence that time is related to temperature at the confidence level of 0.05? If so, describe the relationship in a sentence, if not discuss what your next steps in this analysis might be.**

```{r}
minntemp_slm <- lm(Temp ~ Time, data = minntemp)
summary(minntemp_slm)
```

No, there is not enough statistical evidence that time is related to temperature at a confidence level of 0.05. We fail to reject the null. A next temp would to be to go in and detect any outliers and see what you can do about cleaning up the data in the pre-processing stage until your assumptions pass.

<!--chapter:end:af_lab_4.Rmd-->

---
title: "af_lab_5"
author: "Hannah Wasson"
date: "2024-07-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Questions

## 1. Data were collected in an effort to determine whether different dose levels of a given drug have an effect on blood pressure for people with one of three types of heart disease. To obtain this data, submit the following code:

```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(viridis)
library(RColorBrewer)
library(purrr)
drug <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/drug.csv')
str(drug)
```

### a. Examine the data with a side by side bar chart. Put BloodP on the Y axis, DrugDose on the X axis, and stratify by Disease. What information can you obtain from looking at the data?

y is the mean of blood pressure because you are stratifying by disease!!


```{r}

drug_plot <- drug %>% 
  group_by(DrugDose, Disease) %>%
  dplyr::summarise(mean = mean(BloodP), 
                   sd = sd(BloodP), 
                   max = max(BloodP), 
                   min = min(BloodP))

# Use the grouped data to plot the side-by-side bar chart for change in blood pressure
ggplot(data = drug_plot, aes(x = DrugDose, y = mean, fill = Disease)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(y = "Change in Blood Pressure", x = "Drug Dose Category") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()

```

### b. Run a 2-Way ANOVA, making sure to include an interaction term if the graphical analyses that you performed previously indicate that might be advisable (HINT: Make sure DrugDose is a factor in your model since it is stored as a number). What conclusions can you reach at this point?

The interaction between DrugDose on BloodP is influenced by the interaction of disease. The interaction of DrugDose and Disease is influenced by the BloodP.

```{r}
aov_drug <- aov(BloodP ~ DrugDose * factor(Disease), data = drug)
summary(aov_drug)
```

### c. If an interaction is required in the previous piece, investigate the differences in drug dose by performing a sliced ANOVA across the levels of heart disease.

 Different diseases have an impact on how drug dose impacts blood pressure. 

```{r}
slice_aov_drug <- drug %>%
  group_by(Disease) %>%
  nest()%>%
  mutate(aov = map(data, ~summary(aov(BloodP ~ DrugDose, data = .x))))

print(slice_aov_drug$aov)

```

### 2. A computer service center has four technicians who specialize in repairing three brands of computer disk drives for desktop computers. The service center wants to study the effects of the technician and brand of the disk drive on the service time. To obtain this data, submit the following code:

```{r}
disks <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/disks.csv')
str(disks)
```

### a. Generate a 2-Way ANOVA with Time as the dependent variable and Technician and Brand as the independent variables (HINT: Make sure Brand is a factor in your model since it is stored as a number). Include the interaction between the independent variables in your model. Assume a level of significance of 0.05. Is the overall F test significant in your model? Is there a significant interaction?

At an alpha level of 0.05 we would reject the null hypothesis and conclude that the interaction between technician and time is influenced by the brand and vice versa.


```{r}

aov_disks <- aov(Time ~ Technician * factor(Brand), data = disks)
summary(aov_disks)



```

### b. Is it appropriate to examine the tests for the main effects shown in the output?

No, it is not because once you're interaction term is significant, you don't pay attention to any other effects within the model. 

### c. Determine whether there are differences between the technicians for each brand of disk drive using sliced ANOVA across the brands of disk drives. Also, examine the Tukey HSD among the values of Technician within each slice of Brand. What are your conclusions?

```{r}
brand_aov <- disks %>%
  group_by(Brand) %>%
  nest() %>%
  mutate(aov = map(data, ~summary(aov(Time ~ Technician, data = .x))))

print(brand_aov$aov)

brand_aov_2 <- disks %>%
  group_by(Brand) %>%
  nest() %>%
  mutate(aov = map(data, ~aov(Time ~ Technician, data = .x)), tukey = map(aov, ~TukeyHSD(.)))

print(brand_aov_2$tukey)

```


<!--chapter:end:af_lab_5.Rmd-->

---
title: "af_lab_6"
author: "Hannah Wasson"
date: "2024-07-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Multiple Linear Regression Lab 6

## Cars Dataset (Question 1): 

This dataset has variables pertaining to fuel economy of various cars. Do not create a training and test set. Just use the whole cars2010 dataset for the following analysis. The cars2011 and cars2012 datasets will be used at later time periods

Perform the following analysis:

```{r}
library(AppliedPredictiveModeling)
data(FuelEconomy)
cars <- cars2010

str(cars)
```


### a. Run a regression predicting the FE variable using all the remaining variables. Some of these predictor variables are coded as numeric, but should be treated as categorical. The only numeric variables in your dataset should be EngDispl. All remaining variables are categorical.

```{r}
cars_mlr_1 <- lm(FE ~ EngDispl + factor(NumCyl) + factor(Transmission) + factor(AirAspirationMethod) + factor(NumGears) + factor(TransLockup) + factor(TransCreeperGear) + factor(DriveDesc) + factor(IntakeValvePerCyl) + factor(ExhaustValvesPerCyl) + factor(CarlineClassDesc) + factor(VarValveTiming) + factor(VarValveLift), data = cars)

summary(cars_mlr_1)


```

#### a. Perform a Global F-test. What is your conclusion?

The global F-test is: 
F-statistic: 95.55 on 55 and 1051 DF,  p-value: < 2.2e-16

Reject the null and conclude that at least one of the variables has a Beta that is significantly different than 0. 

#### b. What percent of variation in fuel economy (FE) is explained by these 13 variables?

Multiple R-squared:  0.8333

With a R-Squared of 0.8333 this means that 83.33% of the variation in fuel economy (FE) is explained by the 13 variables. 

### b. Trying to evaluate categorical variables in traditional linear regression output can be difficult because the p-values are for each categorical dummy variable. To evaluate the inclusion of a variable as a whole, you need a global p-value for each categorical variable.

#### a. Use the “car::Anova” function in R on your linear regression object to get the p-values for each categorical variable.

```{r}
car::Anova(cars_mlr_1)
```

#### b. Which of the variables has the highest p-value?

the highest p-values are factor(IntakeValvePerCyl) and factor(VarValveTiming) and both are not signficiant at the 0.05 level. Highest is factor(VarValveTiming) 


### c. Rerun the preceding model, but remove the variable with the highest p-value that you found with the “car::Anova” function. Compare the output with the preceding model.

```{r}
cars_mlr_2 <- lm(FE ~ EngDispl + factor(NumCyl) + factor(Transmission) + factor(AirAspirationMethod) + factor(NumGears) + factor(TransLockup) + factor(TransCreeperGear) + factor(DriveDesc) + factor(IntakeValvePerCyl) + factor(ExhaustValvesPerCyl) + factor(CarlineClassDesc) + factor(VarValveLift), data = cars)

print(car::Anova(cars_mlr_2))
print(summary(cars_mlr_2))
```


#### a. Did the p-value for the model change notably?

p-value previously: p-value: < 2.2e-16
p-value now: p-value: < 2.2e-16

No, the p-value didn't change notably. 


#### b. Did the R-square and adjusted R-square values change notably?

r-squared before: 0.8333
adjusted r-squared before: 0.8246

r-squared after: 0.8333
adjusted r-squared after: 0.8247

No, neither r-squared nor adjusted R-squared changed notably. 

#### c. Did the p-values on other variables change notably?

No, p-values didn't change notably. 

### d. Again, rerun the preceding model (from question c), but eliminate the variable with the highest p-value. Repeat this process of eliminating one variable at a time and rerunning the regression until you only have variables significant at the 0.008 level. Remember to run the model after EACH variable you remove as the p-value might change by removing a variable.


```{r}
cars_mlr_3 <- lm(FE ~ EngDispl + factor(NumCyl) + factor(Transmission) + factor(AirAspirationMethod) + factor(NumGears) + factor(TransCreeperGear) + factor(DriveDesc) + factor(ExhaustValvesPerCyl) + factor(CarlineClassDesc), data = cars)

print(car::Anova(cars_mlr_3))
print(summary(cars_mlr_3))
```



#### a. Did the R-square and adjusted R-square values change notably?

The R-squared value: 0.8303 
The Adjusted R-Squared Value: 0.8222

Yes, the adjusted r-squared changed because we dropped some variables that weren't significant but still a good amount of the model is explained by the variables included (r-squared)


#### b. How many variables did you have left that were significant at the 0.008 level?

There were 9 out of the 13 variables left after removing the insignificant ones at the 0.008 level. 


<!--chapter:end:af_lab_6.Rmd-->

---
title: "af_lab_7"
author: "Hannah Wasson"
date: "2024-07-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Questions 

## 1. Use the same cars2010 dataset you have used in previous labs. To obtain this data, submit the following code:

```{r}
library(AppliedPredictiveModeling)
data(FuelEconomy)
```

## This dataset has variables pertaining to fuel economy of various cars. Do not create a training and test set. Just use the whole cars2010 dataset for the following analysis. The cars2011 and cars2012 datasets will be used at later time periods.

```{r}
cars <- cars2010
str(cars)
```


## Perform the following analysis:

### a. Build a regression model predicting the FE variable using all the remaining variables.Some of these predictor variables are coded as numeric, but should be treated as categorical. The only numeric variables in your dataset should be EngDispl. All remaining variables are categorical. Use forward selection with a 0.1 p-value selection criterion.

```{r}
empty_model <- lm(FE ~ 1, data = cars)
full_model <- lm(FE ~ EngDispl + factor(NumCyl) + factor(Transmission) + factor(AirAspirationMethod) + factor(NumGears) + factor(TransLockup) +
                   factor(TransCreeperGear) + factor(DriveDesc) + factor(IntakeValvePerCyl) + factor(ExhaustValvesPerCyl) + factor(CarlineClassDesc) +
                   + factor(VarValveTiming) + factor(VarValveLift), data = cars)
for_model <- step(empty_model, 
                  scope = list(lower = empty_model, 
                               upper = full_model), 
                  direction = "forward", k = qchisq(0.1, 1, lower.tail =  FALSE))
```


#### a. What is the final model?

FE ~ EngDispl + factor(CarlineClassDesc) + factor(NumCyl) + factor(DriveDesc) + 
    factor(Transmission) + factor(IntakeValvePerCyl) + factor(VarValveLift) + 
    factor(AirAspirationMethod) + factor(NumGears) + factor(TransLockup) + 
    factor(TransCreeperGear) + factor(ExhaustValvesPerCyl)

#### b. What was the first variable added?

FE ~ EngDispl

#### c. What was the last variable added?

factor(ExhaustValvesPerCyl)

### b. Instead of the previous approach, now use stepwise selection with a BIC criterion.

```{r}

step_model_bic <- step(empty_model, 
                  scope = list(lower = empty_model, 
                               upper = full_model), 
                  direction = "both", k = log(nrow(cars)))

```


#### a. How many variables (not parameter estimates) are left?

5 variables are left 


#### b. Are these the same variables as with the forward selection?

Final Model (step BIC): 

FE ~ EngDispl + factor(DriveDesc) + factor(CarlineClassDesc) + 
    factor(NumCyl) + factor(ExhaustValvesPerCyl) + factor(NumGears) + 
    factor(AirAspirationMethod) + factor(TransCreeperGear)
    
    
Final Model (forward p-val): 

FE ~ EngDispl + factor(CarlineClassDesc) + factor(NumCyl) + factor(DriveDesc) + 
    factor(Transmission) + factor(IntakeValvePerCyl) + factor(VarValveLift) + 
    factor(AirAspirationMethod) + factor(NumGears) + factor(TransLockup) + 
    factor(TransCreeperGear) + factor(ExhaustValvesPerCyl)


<!--chapter:end:af_lab_7.Rmd-->

---
title: "af_lab_8"
author: "Hannah Wasson"
date: "2024-07-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
cafe <- read.csv('~/Documents/lab8.csv')
str(cafe)
library(ggplot2)
```
## 1. Perform a simple linear regression with Sales as the response and Dispensers as the predictor variable. What do you see in the residual versus predicted plot? What would you do to fix this problem?

There is a quadratic relationship 

```{r}
disp.s <- scale(cafe$Dispensers, scale = F)
lm <- lm(Sales ~ Dispensers, data = cafe)
lm.q <- lm(Sales ~ I(disp.s ^ 2) + disp.s, data = cafe)

plot(lm)
plot(lm.q)
```
## 2. Perform a forward selection (by hand) using the AIC criteria (you will need to use the command AIC(model) to get the AIC values for each model). The “smallest” model should be the just the intercept. The “biggest” model should be Dispensers up to the power of 4 (be sure to follow model hierarchy). What was the best degree for the polynomial based on AIC?

```{r}
lm.cub <- lm(Sales ~ I(disp.s ^ 3) + I(disp.s ^ 2) + disp.s, data = cafe)
lm.poly <- lm(Sales ~ I(disp.s ^ 4) + I(disp.s ^ 3) + I(disp.s^2) + disp.s, data = cafe)


AIC(lm)
AIC(lm.q)
AIC(lm.cub)
AIC(lm.poly)

```
## 3. Run the model you selected in #2 and look at the residual versus predicted plot. What do you see?

### a. Does the residual plot look random?

Yes, the residual plot seems to look random with equal variance 

### b. Does the assumption of homoscedasticity of the variance appear to hold here?

Yes, according to visual there seems to be homoscedasicity and verifying using Spearman Rank Sum test, we fail to reject the null hypothesis and conclude that there is a linear association in the model. 

```{r}
ggplot(data = lm.q, aes(x = .fitted, y = .resid)) + geom_point(color = 'purple') + geom_hline(yintercept = 0) 

cor.test(abs(resid(lm.q)), fitted.values(lm.q), method = 'spearman', exact = T)

```


<!--chapter:end:af_lab_8.Rmd-->

---
title: "af_lab_9"
author: "Hannah Wasson"
date: "2024-07-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Diagnostic Testing 

## 1. Using the cars2010 data set, run the regression with the following explanatory variables: EngDispl, Transmission, AirAspirationMethod, TransLockup, TransCreeperGear, DriveDesc, IntakeValvePerCyl, CarlineClassDesc, VarValveLift

```{r}
library(ggplot2)
library(tidyverse)


```


### a. Use plots to identify potential influential observations based on the suggested cutoff values.

Using DFFITS ...

```{r}
n.index=seq(1,nrow(cars)) 
cars=cbind(cars,n.index)

lm_full <- lm(FE ~ EngDispl + Transmission + AirAspirationMethod + TransLockup + TransCreeperGear + DriveDesc + IntakeValvePerCyl + CarlineClassDesc + VarValveLift, data = cars)

resid_full_mod <- tibble(residuals(lm_full))
resid_full_mod %>% arrange(desc(resid_full_mod))


length(coefficients(lm_full))

df.cut=2*(sqrt((length(coefficients(lm_full)))/nrow(cars)))

ggplot(lm_full,aes(x=n.index,y=dffits(lm_full))) + geom_point(color="orange")+geom_line(y=df.cut)+geom_line(y=-df.cut)+labs(title = "DFFITS",x="Observation",y="DFFITS")


```

### b. Are there any observations with a dffits larger than 1 AND studentized residuals larger than 3 in magnitude? If so, list the observations.

```{r}
dffits_tib <- tibble(dffits = dffits(lm_full))
dffits_tib %>%  filter(dffits > 1) %>% arrange(desc(dffits))

```


Studentized Residuals:

```{r}

hat.cut=2*(length(coefficients(lm_full)))/nrow(cars)

ggplot(lm_full,aes(x=n.index,y =hatvalues(lm_full)))+geom_point(color="blue")+geom_line(y=hat.cut)+labs(title = "Hat values",x="Observation",y="Hat Values") 

ggplot(lm_full,aes(x=hatvalues(lm_full),y=rstudent(lm_full)))+geom_point(color="orange")+geom_line(y=-3)+geom_line(y=3)+labs(title = "External Studentized Residuals",x="Hat values",y="Residuals") + geom_vline(xintercept = hat.cut)

student_tib <- tibble(student_res = rstudent(lm_full))
student_gr <- student_tib %>%  filter(student_res > 3) %>% arrange(desc(student_res)) %>% count()
student_ls <- student_tib %>%  filter(student_res < -3) %>% arrange((student_res)) %>% count()

ggplot(lm.model,aes(x=n.index,y=rstudent(lm.mod
el)))+geom_point(color="orange")+geom_line(y=-
3)+geom_line(y=3)+labs(title = "External
Studentized
Residuals",x="Observation",y="Residuals")

print(student_gr + student_ls)
```


```{r}
cooks.cut=4/(nrow(cars2010)-length(coefficients(lm_full))-1)
ggplot(lm_full,aes(x=n.index,y=cooks.distance(lm_full)))+geom_point(color="orange")+geom_line(y=cooks.cut)+labs(title = "Cook's D",x="Observation",y="Cook's Distance")

cooks_tib <- tibble(cooks_tib_val = cooks.distance(lm_full))
cooks_tib %>% arrange(desc(cooks_tib_val)) %>% signif(digits=3)
```


<!--chapter:end:af_lab_9.Rmd-->

